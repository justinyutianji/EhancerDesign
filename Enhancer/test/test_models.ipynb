{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.activation as activation\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('../model')  \n",
    "from model import ExpActivation, Unsqueeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNetDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = nn.Conv1d(4, 100, 19)\n",
    "bn1 = nn.BatchNorm1d(100)\n",
    "rl1 = activation.ReLU()\n",
    "mp1 = nn.MaxPool1d(3, 3)\n",
    "\n",
    "c2 = nn.Conv1d(100, 200, 11)\n",
    "bn2 = nn.BatchNorm1d(200)\n",
    "rl2 = activation.ReLU()\n",
    "mp2 = nn.MaxPool1d(3, 3)\n",
    "\n",
    "c3 = nn.Conv1d(200, 200, 7)\n",
    "bn3 = nn.BatchNorm1d(200)\n",
    "rl3 = activation.ReLU()\n",
    "mp3 = nn.MaxPool1d(4, 4) \n",
    "\n",
    "c7 = nn.Conv1d(200, 400, 4)\n",
    "bn7 = nn.BatchNorm1d(400)\n",
    "rl7 = activation.ReLU()\n",
    "mp7 = nn.MaxPool1d(4, 4) \n",
    "\n",
    "# Block 4 : Fully Connected 1 :\n",
    "d4 = nn.Linear(800, 800)  # 1000 for 200 input size\n",
    "bn4 = nn.BatchNorm1d(800, 1e-05, 0.1, True)\n",
    "rl4 = activation.ReLU()\n",
    "dr4 = nn.Dropout(0.3)\n",
    "\n",
    "# Block 5 : Fully Connected 2 :\n",
    "d5 = nn.Linear(800, 800)\n",
    "bn5 = nn.BatchNorm1d(800, 1e-05, 0.1, True)\n",
    "rl5 = activation.ReLU()\n",
    "dr5 = nn.Dropout(0.3)\n",
    "\n",
    "# Block 6 :4Fully connected 3\n",
    "num_classes = 1\n",
    "d6 = nn.Linear(800, num_classes)\n",
    "# self.sig = activation.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: torch.Size([10, 100, 590])\n",
      "Layer 1: torch.Size([10, 100, 196])\n",
      "Layer 2: torch.Size([10, 200, 186])\n",
      "Layer 2: torch.Size([10, 200, 62])\n",
      "Layer 3: torch.Size([10, 200, 56])\n",
      "Layer 3: torch.Size([10, 200, 14])\n",
      "Layer 3.5: torch.Size([10, 400, 11])\n",
      "Layer 3.5: torch.Size([10, 400, 2])\n",
      "o shape is torch.Size([10, 800])\n",
      "Layer 4: torch.Size([10, 800])\n",
      "Layer 4: torch.Size([10, 800])\n",
      "Layer 5: torch.Size([10, 800])\n",
      "Layer 5: torch.Size([10, 800])\n",
      "Finalz: torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10,4,608)\n",
    "x = rl1(bn1(c1(x)))\n",
    "print(f\"Layer 1: {x.shape}\")\n",
    "x = mp1(x)\n",
    "print(f\"Layer 1: {x.shape}\")\n",
    "\n",
    "\n",
    "x = rl2(bn2(c2(x)))\n",
    "print(f\"Layer 2: {x.shape}\")\n",
    "x = mp2(x)\n",
    "print(f\"Layer 2: {x.shape}\")\n",
    "\n",
    "\n",
    "x = rl3(bn3(c3(x)))\n",
    "print(f\"Layer 3: {x.shape}\")\n",
    "x = mp3(x)\n",
    "print(f\"Layer 3: {x.shape}\")\n",
    "\n",
    "x = rl7(bn7(c7(x)))\n",
    "print(f\"Layer 3.5: {x.shape}\")\n",
    "x = mp7(x)\n",
    "print(f\"Layer 3.5: {x.shape}\")\n",
    "\n",
    "o = torch.flatten(x, start_dim=1)\n",
    "print(f\"o shape is {o.shape}\")\n",
    "o = rl4(bn4(d4(o)))\n",
    "print(f\"Layer 4: {o.shape}\")\n",
    "o = dr4(o)\n",
    "print(f\"Layer 4: {o.shape}\")\n",
    "\n",
    "\n",
    "o = rl5(bn5(d5(o)))\n",
    "print(f\"Layer 5: {o.shape}\")\n",
    "o = dr5(o)\n",
    "print(f\"Layer 5: {o.shape}\")\n",
    "\n",
    "\n",
    "o = d6(o)\n",
    "print(f\"Finalz: {o.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11%2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DanQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DanQ(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of DanQ (PMID: 27084946)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_length, num_classes, weight_path=None):\n",
    "        \"\"\"\n",
    "        :param input_length: int, input sequence length\n",
    "        :param num_classes: int, number of output classes\n",
    "        :param weight_path: string, path to the file with model weights\n",
    "        \"\"\"\n",
    "        super(DanQ, self).__init__()\n",
    "\n",
    "        self._options = {\n",
    "            \"input_length\": input_length,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"weight_path\": weight_path\n",
    "        }\n",
    "\n",
    "        self.conv1 = nn.Conv1d(4, 320, kernel_size=26)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.maxp1 = nn.MaxPool1d(kernel_size=13, stride=13)\n",
    "\n",
    "        self.bi_lstm_layer = nn.LSTM(320, 320, num_layers=1,\n",
    "                                     batch_first=True, bidirectional=True)\n",
    "\n",
    "        self._in_features_L1 = math.floor((input_length - 25) / 13.) * 640\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self._in_features_L1, 925),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(925, num_classes),\n",
    "        )\n",
    "\n",
    "        if weight_path:\n",
    "            self.load_state_dict(torch.load(weight_path))\n",
    "\n",
    "    def forward(self, input):      \n",
    "        x = self.act1(self.conv1(input))\n",
    "        x = nn.Dropout(0.2)(self.maxp1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.bi_lstm_layer(x)\n",
    "        x = x.contiguous().view(-1, self._in_features_L1)\n",
    "        x = nn.Dropout(0.5)(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Usage of the DanQ model\n",
    "input_length = 608\n",
    "num_classes = 1\n",
    "\n",
    "# Create an instance of the model\n",
    "model = DanQ(input_length, num_classes)\n",
    "\n",
    "# Prepare input tensor (10 samples, 4 channels, input_length of 608)\n",
    "x = torch.randn(10, 4, input_length)\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = model(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExplaiNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplaiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The ExplaiNN model (PMID: 37370113)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_cnns, input_length, num_classes, filter_size=19, num_fc=2, pool_size=7, pool_stride=7,\n",
    "                 weight_path=None):\n",
    "        \"\"\"\n",
    "        :param num_cnns: int, number of independent cnn units\n",
    "        :param input_length: int, input sequence length\n",
    "        :param num_classes: int, number of outputs\n",
    "        :param filter_size: int, size of the unit's filter, default=19\n",
    "        :param num_fc: int, number of FC layers in the unit, default=2\n",
    "        :param pool_size: int, size of the unit's maxpooling layer, default=7\n",
    "        :param pool_stride: int, stride of the unit's maxpooling layer, default=7\n",
    "        :param weight_path: string, path to the file with model weights\n",
    "        \"\"\"\n",
    "        super(ExplaiNN, self).__init__()\n",
    "\n",
    "        self._options = {\n",
    "            \"num_cnns\": num_cnns,\n",
    "            \"input_length\": input_length,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"filter_size\": filter_size,\n",
    "            \"num_fc\": num_fc,\n",
    "            \"pool_size\": pool_size,\n",
    "            \"pool_stride\": pool_stride,\n",
    "            \"weight_path\": weight_path\n",
    "        }\n",
    "\n",
    "        if num_fc == 0:\n",
    "            self.linears = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=4 * num_cnns, out_channels=1 * num_cnns, kernel_size=filter_size,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(num_cnns),\n",
    "                ExpActivation(),\n",
    "                nn.MaxPool1d(input_length - (filter_size-1)),\n",
    "                nn.Flatten())\n",
    "        elif num_fc == 1:\n",
    "            self.linears = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=4 * num_cnns, out_channels=1 * num_cnns, kernel_size=filter_size,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(num_cnns),\n",
    "                ExpActivation(),\n",
    "                nn.MaxPool1d(pool_size, pool_stride),\n",
    "                nn.Flatten(),\n",
    "                Unsqueeze(),\n",
    "                nn.Conv1d(in_channels=int(((input_length - (filter_size-1)) - (pool_size-1)-1)/pool_stride + 1) * num_cnns,\n",
    "                          out_channels=1 * num_cnns, kernel_size=1,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(1 * num_cnns, 1e-05, 0.1, True),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten())\n",
    "        elif num_fc == 2:\n",
    "            self.linears = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=4 * num_cnns, out_channels=1 * num_cnns, kernel_size=filter_size,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(num_cnns),\n",
    "                ExpActivation(),\n",
    "                nn.MaxPool1d(pool_size, pool_stride),\n",
    "                nn.Flatten(),\n",
    "                Unsqueeze(),\n",
    "                nn.Conv1d(in_channels=int(((input_length - (filter_size-1)) - (pool_size-1)-1)/pool_stride + 1) * num_cnns,\n",
    "                          out_channels=100 * num_cnns, kernel_size=1,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(100 * num_cnns, 1e-05, 0.1, True),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Conv1d(in_channels=100 * num_cnns,\n",
    "                          out_channels=1 * num_cnns, kernel_size=1,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(1 * num_cnns, 1e-05, 0.1, True),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten())\n",
    "        else:\n",
    "            self.linears = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=4 * num_cnns, out_channels=1 * num_cnns, kernel_size=filter_size,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(num_cnns),\n",
    "                ExpActivation(),\n",
    "                nn.MaxPool1d(pool_size, pool_stride),\n",
    "                nn.Flatten(),\n",
    "                Unsqueeze(),\n",
    "                nn.Conv1d(in_channels=int(((input_length - (filter_size-1)) - (pool_size-1)-1)/pool_stride + 1) * num_cnns,\n",
    "                          out_channels=100 * num_cnns, kernel_size=1,\n",
    "                          groups=num_cnns),\n",
    "                nn.BatchNorm1d(100 * num_cnns, 1e-05, 0.1, True),\n",
    "                nn.ReLU())\n",
    "\n",
    "            self.linears_bg = nn.ModuleList([nn.Sequential(nn.Dropout(0.3),\n",
    "                                                           nn.Conv1d(in_channels=100 * num_cnns,\n",
    "                                                                     out_channels=100 * num_cnns, kernel_size=1,\n",
    "                                                                     groups=num_cnns),\n",
    "                                                           nn.BatchNorm1d(100 * num_cnns, 1e-05, 0.1, True),\n",
    "                                                           nn.ReLU()) for i in range(num_fc - 2)])\n",
    "\n",
    "            self.last_linear = nn.Sequential(nn.Dropout(0.3),\n",
    "                                             nn.Conv1d(in_channels=100 * num_cnns, out_channels=1 * num_cnns,\n",
    "                                                       kernel_size=1,\n",
    "                                                       groups=num_cnns),\n",
    "                                             nn.BatchNorm1d(1 * num_cnns, 1e-05, 0.1, True),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Flatten())\n",
    "\n",
    "        self.final = nn.Linear(num_cnns, num_classes)\n",
    "\n",
    "        if weight_path:\n",
    "            self.load_state_dict(torch.load(weight_path))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(1, self._options[\"num_cnns\"], 1)\n",
    "        print(f\"x.repeat {x.shape}\")\n",
    "        if self._options[\"num_fc\"] <= 2:\n",
    "            outs = self.linears(x)\n",
    "        else:\n",
    "            outs = self.linears(x)\n",
    "            for i in range(len(self.linears_bg)):\n",
    "                outs = self.linears_bg[i](outs)\n",
    "            outs = self.last_linear(outs)\n",
    "        print(f\"outs {outs.shape}\")\n",
    "        out = self.final(outs)\n",
    "        print(f\"out {out.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e5\n",
      "5e5\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in [1e-5, 5e-5]:\n",
    "    print(str(learning_rate)[:2] + str(learning_rate)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.repeat torch.Size([10, 20, 608])\n",
      "outs torch.Size([10, 5])\n",
      "out torch.Size([10, 2])\n",
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "# Usage of the DanQ model\n",
    "input_length = 608\n",
    "num_classes = 1\n",
    "\n",
    "model = ExplaiNN(num_cnns = 5, input_length = 608, num_classes = 2)\n",
    "\n",
    "# Prepare input tensor (10 samples, 4 channels, input_length of 608)\n",
    "x = torch.randn(10, 4, input_length)\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = model(x)\n",
    "\n",
    "# Print the output shape\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explainn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
